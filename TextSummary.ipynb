{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"e5fGa44-3zA4"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import time\n","import re\n","import pickle"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"QXETV7w-2Y2s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd /content/drive/My Drive/ML/ML Innovative"],"metadata":{"id":"BRYqIegK7clW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_excel('news.xlsx')\n","df"],"metadata":{"id":"aNI4EbGD4hOx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.drop(['Source ', 'Time ', 'Publish Date'], axis=1, inplace=True)\n","df.head()"],"metadata":{"id":"JA8EYuRC8YS1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.shape"],"metadata":{"id":"4GG5Ssq18l24"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text=df['Short']\n","summary=df['Headline']"],"metadata":{"id":"4ag9wFxO8saz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#adding delimiters in summary\n","summary=summary.apply(lambda x: '<start>'+x+'<stop>')\n","summary.head()"],"metadata":{"id":"-ka1mrOO9KL2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#removing filters\n","filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n","oov_token = '<unk>'"],"metadata":{"id":"khO14j8L_cm7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n","summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)"],"metadata":{"id":"sbJcDq0ZBXLK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_tokenizer.fit_on_texts(text)\n","summary_tokenizer.fit_on_texts(summary)"],"metadata":{"id":"s1e04DSWBgQ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# saving\n","with open('text_tokenizer.pickle', 'wb') as handle:\n","    pickle.dump(text_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"metadata":{"id":"DIPm7YqYGofw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#returns list\n","input=text_tokenizer.texts_to_sequences(text)\n","output=summary_tokenizer.texts_to_sequences(summary)"],"metadata":{"id":"8p78XZFaH3NX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary_tokenizer.texts_to_sequences([\"This rick is a rick\"])"],"metadata":{"id":"MpyZkLTHB7-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary_tokenizer.sequences_to_texts([[172, 1, 21, 10, 1]])"],"metadata":{"id":"KGIAkS9XB-hO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder_vocab_size = len(text_tokenizer.word_index) + 1\n","decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n","\n","# vocab_size\n","encoder_vocab_size, decoder_vocab_size"],"metadata":{"id":"XKtBsRHBCFjR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_lengths = pd.Series([len(x) for x in text])\n","summary_lengths = pd.Series([len(x) for x in summary])"],"metadata":{"id":"lPvZDXTdFj9M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_lengths.describe()"],"metadata":{"id":"vNA1D_hIGRED"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# summary_lengths.describe(document_lengths = pd.Series([len(x) for x in document]))\n","summary_lengths = pd.Series([len(x) for x in summary])"],"metadata":{"id":"-IsP1QZuGWaf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder_maxlen=400#75th percentile from text_lengths.describe()\n","decoder_maxlen=70#75th percentile from summary_lengths"],"metadata":{"id":"1htTM-KVGhY-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#padding and truncating to obtain n d matrix for both input and output\n","input = tf.keras.preprocessing.sequence.pad_sequences(input, maxlen=encoder_maxlen, padding='post', truncating='post')\n","output = tf.keras.preprocessing.sequence.pad_sequences(output, maxlen=decoder_maxlen, padding='post', truncating='post')"],"metadata":{"id":"swTivdYjG5xY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input = tf.cast(input, dtype = tf.int32)\n","output = tf.cast(output, dtype = tf.int32)"],"metadata":{"id":"fYxEpzuVIwY-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BUFFER_SIZE = 20000\n","BATCH_SIZE = 64"],"metadata":{"id":"-VO2Z997JmMl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = tf.data.Dataset.from_tensor_slices((input, output)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"],"metadata":{"id":"PZglgJaaJqv_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_angles(position, i, d_model):\n","    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n","    return position * angle_rates"],"metadata":{"id":"7NtGuBd6r1Jp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def positional_encoding(position, d_model):\n","    angle_rads = get_angles(\n","        np.arange(position)[:, np.newaxis],\n","        np.arange(d_model)[np.newaxis, :],\n","        d_model\n","    )\n","\n","    # apply sin to even indices in the array; 2i\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","\n","    # apply cos to odd indices in the array; 2i+1\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","    pos_encoding = angle_rads[np.newaxis, ...]\n","\n","    return tf.cast(pos_encoding, dtype=tf.float32)"],"metadata":{"id":"uejVS9TRr1UY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_look_ahead_mask(size):\n","    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","    return mask\n","\n","def create_padding_mask(seq):\n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","    return seq[:, tf.newaxis, tf.newaxis, :]\n"],"metadata":{"id":"igvZIx0KuKoG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Scaled Dot Product\n","\n","def scaled_dot_product_attention(q, k, v, mask):\n","    matmul_qk = tf.matmul(q, k, transpose_b=True)\n","\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","    if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)\n","\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n","\n","    output = tf.matmul(attention_weights, v)\n","    return output, attention_weights"],"metadata":{"id":"N6rZ3ab5J5vf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#MultiHeadedAttention\n","\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        assert d_model % self.num_heads == 0\n","\n","        self.depth = d_model // self.num_heads\n","\n","        self.wq = tf.keras.layers.Dense(d_model)\n","        self.wk = tf.keras.layers.Dense(d_model)\n","        self.wv = tf.keras.layers.Dense(d_model)\n","\n","        self.dense = tf.keras.layers.Dense(d_model)\n","\n","    def split_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def call(self, v, k, q, mask):\n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q)\n","        k = self.wk(k)\n","        v = self.wv(v)\n","\n","        q = self.split_heads(q, batch_size)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","\n","        scaled_attention, attention_weights = scaled_dot_product_attention(\n","            q, k, v, mask)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","\n","        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n","        output = self.dense(concat_attention)\n","\n","        return output, attention_weights"],"metadata":{"id":"8PT1BrP003jT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#linear feed forward network\n","def point_wise_feed_forward_network(d_model, dff):\n","    return tf.keras.Sequential([\n","        tf.keras.layers.Dense(dff, activation='relu'),\n","        tf.keras.layers.Dense(d_model)\n","    ])"],"metadata":{"id":"X3oicyXm12Ms"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#encoder unit\n","class EncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(EncoderLayer, self).__init__()\n","\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, x, training, mask):\n","        attn_output, _ = self.mha(x, x, x, mask)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(x + attn_output)\n","\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        out2 = self.layernorm2(out1 + ffn_output)\n","\n","        return out2"],"metadata":{"id":"0VD-C9j4jr6E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#decoder unit\n","class DecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(DecoderLayer, self).__init__()\n","\n","        self.mha1 = MultiHeadAttention(d_model, num_heads)\n","        self.mha2 = MultiHeadAttention(d_model, num_heads)\n","\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","        self.dropout3 = tf.keras.layers.Dropout(rate)\n","\n","\n","    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n","        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n","        attn1 = self.dropout1(attn1, training=training)\n","        out1 = self.layernorm1(attn1 + x)\n","\n","        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n","        attn2 = self.dropout2(attn2, training=training)\n","        out2 = self.layernorm2(attn2 + out1)\n","\n","        ffn_output = self.ffn(out2)\n","        ffn_output = self.dropout3(ffn_output, training=training)\n","        out3 = self.layernorm3(ffn_output + out2)\n","\n","        return out3, attn_weights_block1, attn_weights_block2\n"],"metadata":{"id":"sbRFQ9MWj1dH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#encoder layer\n","class Encoder(tf.keras.layers.Layer):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n","        super(Encoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n","\n","        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, x, training, mask):\n","        seq_len = tf.shape(x)[1]\n","\n","        x = self.embedding(x)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x, training=training)\n","\n","        for i in range(self.num_layers):\n","            x = self.enc_layers[i](x, training, mask)\n","\n","        return x\n"],"metadata":{"id":"i1EO6JNPkEqq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#decoder layer\n","class Decoder(tf.keras.layers.Layer):\n","    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n","        super(Decoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","\n","        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n","        seq_len = tf.shape(x)[1]\n","        attention_weights = {}\n","\n","        x = self.embedding(x)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x, training=training)\n","\n","        for i in range(self.num_layers):\n","            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n","\n","            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n","            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n","\n","        return x, attention_weights"],"metadata":{"id":"9OD6WLWXkL-E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#transformer layer\n","class Transformer(tf.keras.Model):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n","        super(Transformer, self).__init__()\n","\n","        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n","\n","        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n","\n","        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","\n","    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n","        enc_output = self.encoder(inp, training, enc_padding_mask)\n","\n","        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n","\n","        final_output = self.final_layer(dec_output)\n","\n","        return final_output, attention_weights"],"metadata":{"id":"Doo2y9ExkZEP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hyper-params\n","num_layers = 4\n","d_model = 128\n","dff = 512\n","num_heads = 8\n","EPOCHS = 20"],"metadata":{"id":"p-2sWvjAkfPM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Define a learning rate schedule (you can use the CustomSchedule from your previous code).\n","class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, d_model, warmup_steps=4000):\n","        super(CustomSchedule, self).__init__()\n","\n","        self.d_model = d_model\n","        self.d_model = tf.cast(self.d_model, tf.float32)\n","        self.warmup_steps = warmup_steps\n","\n","    def __call__(self, step):\n","        step=tf.cast(step, dtype=tf.float32)\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps ** -1.5)\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"],"metadata":{"id":"AICSUOogub7-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#defining losses and other metrics\n","learning_rate = CustomSchedule(d_model)\n","# pylint: disable=W0212\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"],"metadata":{"id":"FuJFxy-vkwKE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"],"metadata":{"id":"JWT9aOqblLZc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loss = tf.keras.metrics.Mean(name='train_loss')"],"metadata":{"id":"7oyi--QMlGvF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transformer = Transformer(\n","    num_layers,\n","    d_model,\n","    num_heads,\n","    dff,\n","    encoder_vocab_size,\n","    decoder_vocab_size,\n","    pe_input=encoder_vocab_size,\n","    pe_target=decoder_vocab_size,\n",")"],"metadata":{"id":"N-NNkci5lPxL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create mask\n","def create_masks(inp, tar):\n","    enc_padding_mask = create_padding_mask(inp)\n","    dec_padding_mask = create_padding_mask(inp)\n","\n","    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","    dec_target_padding_mask = create_padding_mask(tar)\n","    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","\n","    return enc_padding_mask, combined_mask, dec_padding_mask"],"metadata":{"id":"ux4L7TB5lUlw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#checkpoints\n","\n","checkpoint_path = \"checkpoints\"\n","\n","ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","if ckpt_manager.latest_checkpoint:\n","    ckpt.restore(ckpt_manager.latest_checkpoint)\n","    print ('Latest checkpoint restored!!')"],"metadata":{"id":"QM4zoYf9mASg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@tf.function\n","def train_step(inp, tar):\n","    tar_inp = tar[:, :-1]\n","    tar_real = tar[:, 1:]\n","\n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","    with tf.GradientTape() as tape:\n","        predictions, _ = transformer(\n","            inp, tar_inp,\n","            True,\n","            enc_padding_mask,\n","            combined_mask,\n","            dec_padding_mask\n","        )\n","        loss = loss_function(tar_real, predictions)\n","\n","    gradients = tape.gradient(loss, transformer.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","\n","    train_loss(loss)\n","    return transformer"],"metadata":{"id":"R6c8ydMcu1nc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(input_document):\n","    input_document = text_tokenizer.texts_to_sequences([input_document])\n","    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n","\n","    encoder_input = tf.expand_dims(input_document[0], 0)\n","\n","    decoder_input = [summary_tokenizer.word_index['<start>']]\n","    output = tf.expand_dims(decoder_input, 0)\n","    transformer=Transformer(\n","    num_layers,\n","    d_model,\n","    num_heads,\n","    dff,\n","    encoder_vocab_size,\n","    decoder_vocab_size,\n","    pe_input=encoder_vocab_size,\n","    pe_target=decoder_vocab_size,\n","    )\n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n","    predictions, attention_weights = transformer(\n","            encoder_input,\n","            output,\n","            False,\n","            enc_padding_mask,\n","            combined_mask,\n","            dec_padding_mask\n","        )\n","    transformer.load_weights(\"model.h5\")\n","    for i in range(decoder_maxlen):\n","        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n","\n","        predictions, attention_weights = transformer(\n","            encoder_input,\n","            output,\n","            False,\n","            enc_padding_mask,\n","            combined_mask,\n","            dec_padding_mask\n","        )\n","\n","        predictions = predictions[: ,-1:, :]\n","        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","        if predicted_id == summary_tokenizer.word_index[\"<stop>\"]:\n","            return tf.squeeze(output, axis=0), attention_weights\n","\n","        output = tf.concat([output, predicted_id], axis=-1)\n","\n","    return tf.squeeze(output, axis=0), attention_weights"],"metadata":{"id":"SXJOV7TOwP37"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def summarize(input_document):\n","    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n","    summarized = evaluate(input_document=input_document)[0].numpy()\n","    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n","    return summary_tokenizer.sequences_to_texts(summarized)[0][3:-3:]  # since there is just one translated document"],"metadata":{"id":"2l32wEuZwSIL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summarize(\n","    \"Mukesh Ambani-led Reliance Industries (RIL) was barred from trading in futures market for a year over stake sale in Reliance Petroleum (RPL). In 2007, RIL sold 4.1% stake in RPL, but shares were first &#39;short-sold&#39; in futures market to avoid a fall in RPL stocks. Short sale means selling shares with plans to buy them back later at lower prices.\"\n",")"],"metadata":{"id":"FiWOA4SrwhR9"},"execution_count":null,"outputs":[]}]}